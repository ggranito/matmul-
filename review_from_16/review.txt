# Nice Stuff
Using `sysconf` to query the parameters on the fly is cool. In the haste to squeeze out every iota of performance, we've only been hardcoding values from back-of-the-envelope calculations, and didn't think to make use of the available syscalls. Doing a formal comparison of GCC and ICC is a nice touch that justifies why we can ignore GCC. Although it's been stated and hinted at, it's still good to see numerical verification.

# Soon-to-be-nice Stuff
Your performance isn't half bad! Once you start twiddling compiler flags, the performance goes up by a lot. That's pretty low hanging fruit considering you have the memory alignment and blocking somewhat thought through already. One of the other groups we reviewed toyed with a rectangular block size and seemed to get pretty decent performance. Thought we would just pass along the idea. See what you can make of it?

Specifics
=========
## Loop Permutations and Compilers
icc appears to handle vectorization better than gcc. To strengthen the case you have made, it might be worthwhile including the vectorization report from icc (`-vec`) that shows the improvements in stride length made by the compiler. In regard to the loop order, we had an identical conclusion for naive matrix multiply, noting that this ordering is not necessarily true when blocking was implemented because the entire multiplication kernel now fits within the cache.

## Tuned Blocked Matrix multiply
To the best of our understanding, the L3 cache is shared across the ring interconnect, which means that an operation on an adjacent core could invalidate the entire cache line. It may be better to use the L2 cache parameters instead when calculating the results. In addition, the upper range of the block sizes you have investigated (1024/2048) appear to come very close (or exceed, in the case of 2048) to the dimensions of the input itself, which negates the effects of blocking.

## Next Stages
We have tried transposing the array and obtained good results. Even a naive transposition is beneficial because it allows one to exploit a different loop order to minimize the stride of the outermost loop. An in-memory transposition will probably yield the best results, but we have yet to try that ourselves.
